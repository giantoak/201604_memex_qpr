{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook\n",
    "# ... Is Super Junky. As long as this note is here, maybe assume\n",
    "# you can't run it.\n",
    "\n",
    "In here, everything on which we're planning to train gets merged together and dumped to a CSV and a pickle for quick reference.\n",
    "\n",
    "This pickle is **keyed on dd_id**. But it's **phone numbers** that are relevant for the classifier.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import chain\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from sqlalchemy import create_engine\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import phone_str_to_dd_format\n",
    "from helpers import disaggregated_df\n",
    "from helpers import aggregated_df\n",
    "from helpers import dummify_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_of_tables_for_dd_ids(dd_ids, sqlite_tables, sql_con):\n",
    "    \"\"\"\n",
    "    :param list dd_ids: list of Deep Dive IDs to retrieve\n",
    "    :param list sqlite_tables: list of SQLite tables to join\n",
    "    :param sqlalchemy.create_engine sql_con: Connection to SQLite (can be \\\n",
    "    omitted)\n",
    "    :returns: `pandas.DataFrame` -- dataframe of tables, joined using the Deep \\\n",
    "    Dive IDs.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    dd_ids_str = ','.join(['\"{}\"'.format(x) for x in dd_ids])\n",
    "    query_fmt = 'select * from {} where dd_id in ({})'.format\n",
    "\n",
    "    df = pd.read_sql(query_fmt(sqlite_tables[0], dd_ids_str), sql_con).drop_duplicates()\n",
    "    df['dd_id'] = df.dd_id.astype(int)\n",
    "\n",
    "    for s_t in sqlite_tables[1:]:\n",
    "        df_2 = pd.read_sql(query_fmt(s_t, dd_ids_str), sql_con)\n",
    "        df_2['dd_id'] = df_2.dd_id.astype(int)\n",
    "        \n",
    "        # We use outer joins because dd_ids in one table may be missing from the other.\n",
    "        df = df.merge(df_2, on=['dd_id'], how='outer')\n",
    "\n",
    "    if 'post_date' in df:\n",
    "        df['post_date'] = df.post_date.apply(pd.to_datetime)\n",
    "        \n",
    "    if 'duration_in_mins' in df:\n",
    "        df['duration_in_mins'] = df.duration_in_mins.apply(lambda x: float(x) if x != '' else np.nan)\n",
    "        \n",
    "    # I melted some rows when making this, and it's proven a mistake. Let's unmelt\n",
    "    melted_cols = ['ethnicity', 'flag']\n",
    "    for m_c in melted_cols:\n",
    "        if m_c in df.columns:\n",
    "            df = aggregated_df(df, m_c, 'dd_id', '|')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsns = [json.loads(x) for x in open('../../data/orig_ht_data/ht_training.json', 'r')]\n",
    "train_df = pd.DataFrame.from_records(jsns)\n",
    "train_df = train_df.ix[:, ['class', 'phone', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4796, 3)\n",
      "(4192, 3)\n",
      "(4169, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df['phone'] = train_df.phone.apply(lambda x:','.join(x))\n",
    "train_df = disaggregated_df(train_df, 'phone', ',')\n",
    "print(train_df.shape)\n",
    "train_df = train_df.ix[train_df.phone != 'nan', :]\n",
    "print(train_df.shape)\n",
    "\n",
    "train_df['phone'] = train_df.phone.apply(phone_str_to_dd_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop phone numbers that have both positive and negative values. This is an oversimplification, and we could also try something like calculating a real score. This is okay for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4034, 3)\n"
     ]
    }
   ],
   "source": [
    "pos_neg_set = \\\n",
    "set(train_df.ix[(train_df['class'] == 'positive'), 'phone']) &\\\n",
    "set(train_df.ix[(train_df['class'] == 'negative'), 'phone'])\n",
    "train_df = train_df.ix[~train_df.phone.isin(pos_neg_set), :]\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using urls, so we drop them too. We can put these back if we want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.ix[:, ['class', 'phone']].drop_duplicates()\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in random ad phone numbers\n",
    "Selected by Feng! 2 million, exclude those in the training data, exclude those with over a thousand associated ads.\n",
    "We're taking these as **negative** examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_phones = open('../../data/lattice/cp1_training_with_random/training_phones_random.txt', 'r').readlines()\n",
    "null_phones = [x.strip() for x in null_phones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, pd.DataFrame({'class': ['negative']*len(null_phones), 'phone': null_phones})])\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DeepDive Data w/ Training Phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_con = create_engine('sqlite:////Users/pmlandwehr/wkbnch/memex/memex_queries/dd_dump.db')\n",
    "query_str_fmt = 'select {} from {} where {} in ({})'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53005, 2)\n",
      "(29027, 2)\n"
     ]
    }
   ],
   "source": [
    "phone_str_list = ','.join(['\"{}\"'.format(x) for x in train_df.phone.unique()])\n",
    "query_str = query_str_fmt('*', 'dd_id_to_phone', 'phone', phone_str_list)\n",
    "df = pd.read_sql(query_str, sql_con)\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28512, 9)\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_of_tables_for_dd_ids(list(df.dd_id.unique()),\n",
    "                                ['dd_id_to_price_duration',\n",
    "                                'dd_id_to_flag',\n",
    "                                'dd_id_to_age',\n",
    "                                'dd_id_to_cbsa',\n",
    "                                'dd_id_to_ethnicity',\n",
    "                                'dd_id_to_cdr_id'],\n",
    "                               sql_con)\n",
    "print(df_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those ads with prices and durations, let's add a price per minute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2['price_per_min'] = df_2.price / df_2.duration_in_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29519, 11)\n"
     ]
    }
   ],
   "source": [
    "## DD IDs in the first list are definitive and complete, so left join\n",
    "df_3 = df.merge(df_2, on='dd_id', how='left')\n",
    "print(df_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Deep Dive Data with Training Data\n",
    "This **very vanilla** join is sort of insufficient because it keepts eveything keyed on `dd_ids`. It should really be keyed on unique phone numbers, with ad-level stats getting aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29531, 12)\n"
     ]
    }
   ],
   "source": [
    "df_4 = train_df.merge(df_3, on='phone', how='left')\n",
    "print(df_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Deep Dive Data with Greg's HT Data\n",
    "## STD Data\n",
    "It looks like MSA, CBSA, and Name all match to the same values, so could drop some colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_df = pd.read_excel('../../data/greg_correlates/std.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSA</th>\n",
       "      <th>CBSA</th>\n",
       "      <th>Name</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Year</th>\n",
       "      <th>Cases</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31000US12060</td>\n",
       "      <td>12060</td>\n",
       "      <td>Atlanta-Sandy Springs-Roswell, GA</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>20337</td>\n",
       "      <td>370.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31000US12420</td>\n",
       "      <td>12420</td>\n",
       "      <td>Austin-Round Rock, TX</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>8456</td>\n",
       "      <td>495.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31000US12580</td>\n",
       "      <td>12580</td>\n",
       "      <td>Baltimore-Columbia-Towson, MD</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>12883</td>\n",
       "      <td>478.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31000US13820</td>\n",
       "      <td>13820</td>\n",
       "      <td>Birmingham-Hoover, AL</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>6120</td>\n",
       "      <td>541.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31000US14460</td>\n",
       "      <td>14460</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>13285</td>\n",
       "      <td>289.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MSA   CBSA                               Name    Disease  Year  \\\n",
       "0  31000US12060  12060  Atlanta-Sandy Springs-Roswell, GA  Chlamydia  2009   \n",
       "1  31000US12420  12420              Austin-Round Rock, TX  Chlamydia  2009   \n",
       "2  31000US12580  12580      Baltimore-Columbia-Towson, MD  Chlamydia  2009   \n",
       "3  31000US13820  13820              Birmingham-Hoover, AL  Chlamydia  2009   \n",
       "4  31000US14460  14460     Boston-Cambridge-Newton, MA-NH  Chlamydia  2009   \n",
       "\n",
       "   Cases   Rate  \n",
       "0  20337  370.2  \n",
       "1   8456  495.9  \n",
       "2  12883  478.8  \n",
       "3   6120  541.1  \n",
       "4  13285  289.5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144121, 17)\n"
     ]
    }
   ],
   "source": [
    "df_4 = df_4.merge(std_df.ix[:, ['Name', 'Disease', 'Year', 'Cases', 'Rate', 'MSA']],\n",
    "                  left_on='area',\n",
    "                  right_on='Name',\n",
    "                  how='left')\n",
    "del df_4['Name']\n",
    "print(df_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSA Characteristics\n",
    "Note that we're using the **yearly** version of the file. We could also use the **monthly** version. I'm primarily choosing yearly because monthly had some import issues that it doesn't seem worth wrangling at this second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msa_df = pd.read_csv('../../data/greg_correlates/msa_characteristics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_msa_code</th>\n",
       "      <th>property</th>\n",
       "      <th>rape</th>\n",
       "      <th>violent</th>\n",
       "      <th>male_wage_mean</th>\n",
       "      <th>male_wage_p05</th>\n",
       "      <th>male_wage_p10</th>\n",
       "      <th>male_wage_p25</th>\n",
       "      <th>male_wage_p50</th>\n",
       "      <th>male_wage_p75</th>\n",
       "      <th>...</th>\n",
       "      <th>wage_sum.wght</th>\n",
       "      <th>female_epop</th>\n",
       "      <th>swnauthemp</th>\n",
       "      <th>swnftemp</th>\n",
       "      <th>ad_p10_msa</th>\n",
       "      <th>ad_p90_msa</th>\n",
       "      <th>ad_mean_msa</th>\n",
       "      <th>ad_median_msa</th>\n",
       "      <th>ad_count_msa</th>\n",
       "      <th>ad_p50_msa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31000US10180</td>\n",
       "      <td>5538.60</td>\n",
       "      <td>63.00</td>\n",
       "      <td>593.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>275.219812</td>\n",
       "      <td>169.981313</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31000US10420</td>\n",
       "      <td>21815.75</td>\n",
       "      <td>273.75</td>\n",
       "      <td>2182.0</td>\n",
       "      <td>30.566309</td>\n",
       "      <td>6.35</td>\n",
       "      <td>9.30</td>\n",
       "      <td>14.8</td>\n",
       "      <td>22.50</td>\n",
       "      <td>35.5</td>\n",
       "      <td>...</td>\n",
       "      <td>363283.0</td>\n",
       "      <td>0.463908</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>206.404629</td>\n",
       "      <td>134.607090</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>4503.0</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31000US10500</td>\n",
       "      <td>6969.60</td>\n",
       "      <td>40.60</td>\n",
       "      <td>961.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>251.629542</td>\n",
       "      <td>150.384258</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31000US10580</td>\n",
       "      <td>21216.20</td>\n",
       "      <td>175.40</td>\n",
       "      <td>2430.0</td>\n",
       "      <td>31.147955</td>\n",
       "      <td>7.00</td>\n",
       "      <td>9.75</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>37.5</td>\n",
       "      <td>...</td>\n",
       "      <td>434363.0</td>\n",
       "      <td>0.472414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>252.108219</td>\n",
       "      <td>165.443290</td>\n",
       "      <td>157.268464</td>\n",
       "      <td>3711.0</td>\n",
       "      <td>157.268464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31000US10740</td>\n",
       "      <td>37465.25</td>\n",
       "      <td>453.25</td>\n",
       "      <td>6314.5</td>\n",
       "      <td>26.628241</td>\n",
       "      <td>5.70</td>\n",
       "      <td>7.50</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.25</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>451024.0</td>\n",
       "      <td>0.400557</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>244.352807</td>\n",
       "      <td>156.882813</td>\n",
       "      <td>144.974193</td>\n",
       "      <td>9978.0</td>\n",
       "      <td>144.974193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  census_msa_code  property    rape  violent  male_wage_mean  male_wage_p05  \\\n",
       "0    31000US10180   5538.60   63.00    593.8             NaN            NaN   \n",
       "1    31000US10420  21815.75  273.75   2182.0       30.566309           6.35   \n",
       "2    31000US10500   6969.60   40.60    961.0             NaN            NaN   \n",
       "3    31000US10580  21216.20  175.40   2430.0       31.147955           7.00   \n",
       "4    31000US10740  37465.25  453.25   6314.5       26.628241           5.70   \n",
       "\n",
       "   male_wage_p10  male_wage_p25  male_wage_p50  male_wage_p75     ...      \\\n",
       "0            NaN            NaN            NaN            NaN     ...       \n",
       "1           9.30           14.8          22.50           35.5     ...       \n",
       "2            NaN            NaN            NaN            NaN     ...       \n",
       "3           9.75           15.0          25.00           37.5     ...       \n",
       "4           7.50           12.0          20.25           34.0     ...       \n",
       "\n",
       "   wage_sum.wght  female_epop  swnauthemp  swnftemp ad_p10_msa  ad_p90_msa  \\\n",
       "0            NaN          NaN       197.0     179.0  80.000000  275.219812   \n",
       "1       363283.0     0.463908      1034.0     975.0  83.333333  206.404629   \n",
       "2            NaN          NaN         NaN       NaN  80.000000  251.629542   \n",
       "3       434363.0     0.472414         NaN       NaN  90.000000  252.108219   \n",
       "4       451024.0     0.400557      1347.0    1208.0  87.500000  244.352807   \n",
       "\n",
       "   ad_mean_msa  ad_median_msa  ad_count_msa  ad_p50_msa  \n",
       "0   169.981313     150.000000        1148.0  150.000000  \n",
       "1   134.607090     115.000000        4503.0  115.000000  \n",
       "2   150.384258     140.000000        1565.0  140.000000  \n",
       "3   165.443290     157.268464        3711.0  157.268464  \n",
       "4   156.882813     144.974193        9978.0  144.974193  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144121, 60)\n"
     ]
    }
   ],
   "source": [
    "df_4 = df_4.merge(msa_df,\n",
    "                  left_on='MSA',\n",
    "                  right_on='census_msa_code',\n",
    "                 how='left')\n",
    "del df_4['census_msa_code']\n",
    "print(df_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del msa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_4.to_csv('../../data/merged/data_to_use_by_ad.csv', index=False)\n",
    "df_4.to_pickle('../../data/merged/data_to_use_by_ad.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
