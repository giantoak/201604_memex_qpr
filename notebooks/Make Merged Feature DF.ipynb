{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook\n",
    "\n",
    "In here, everything on which we're planning to train gets merged together and dumped to a CSV and a pickle for quick reference.\n",
    "\n",
    "------\n",
    "\n",
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import chain\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from sqlalchemy import create_engine\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def phone_str_to_dd_format(phone_str):\n",
    "    \"\"\"\n",
    "    :param str phone_str:\n",
    "    :returns: `str` --\n",
    "    \"\"\"\n",
    "    if len(phone_str) !=  10:\n",
    "        return phone_str\n",
    "    return '({}) {}-{}'.format(phone_str[:3], phone_str[3:6], phone_str[6:])\n",
    "\n",
    "\n",
    "def disaggregated_df(df, aggregate_col, sep):\n",
    "    \"\"\"\n",
    "    DOES NOT save the original index\n",
    "    You could definitely do this faster outside of python, but sometimes that isn't possible\n",
    "    :param pandas.DataFrame df:\n",
    "    :param str aggregate_col:\n",
    "    :param str sep:\n",
    "    :returns: `pandas.DataFrame` -- \n",
    "    \"\"\"\n",
    "    from itertools import chain\n",
    "\n",
    "    good_slice = df.ix[df[aggregate_col].apply(lambda x: x.find(sep) == -1)]\n",
    "    bad_slice = df.ix[df[aggregate_col].apply(lambda x: x.find(sep) > -1)]\n",
    "\n",
    "    def row_breaker(x):\n",
    "        broken_row = []\n",
    "        for new_val in x[aggregate_col].split(sep):\n",
    "            broken_row.append([x[key]\n",
    "                               if key != aggregate_col else new_val\n",
    "                               for key in df.columns])\n",
    "        return broken_row\n",
    "\n",
    "    rows = list(chain(*bad_slice.apply(row_breaker, axis=1).values))\n",
    "    new_df = pd.concat([good_slice, pd.DataFrame.from_records(rows, columns=df.columns)]).drop_duplicates()\n",
    "    new_df.reset_index(inplace=True, drop=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_of_tables_for_dd_ids(dd_ids, sqlite_tables, sql_con):\n",
    "    \"\"\"\n",
    "    :param list dd_ids: list of Deep Dive IDs to retrieve\n",
    "    :param list sqlite_tables: list of SQLite tables to join\n",
    "    :param sqlalchemy.create_engine sql_con: Connection to SQLite (can be \\\n",
    "    omitted)\n",
    "    :returns: `pandas.DataFrame` -- dataframe of tables, joined using the Deep \\\n",
    "    Dive IDs.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    dd_ids_str = ','.join(['\"{}\"'.format(x) for x in dd_ids])\n",
    "    query_fmt = 'select * from {} where dd_id in ({})'.format\n",
    "\n",
    "    df = pd.read_sql(query_fmt(sqlite_tables[0], dd_ids_str), sql_con).drop_duplicates()\n",
    "    df['dd_id'] = df.dd_id.astype(int)\n",
    "\n",
    "    for s_t in sqlite_tables[1:]:\n",
    "        df_2 = pd.read_sql(query_fmt(s_t, dd_ids_str), sql_con)\n",
    "        df_2['dd_id'] = df_2.dd_id.astype(int)\n",
    "        \n",
    "        # We use outer joins because dd_ids in one table may be missing from the other.\n",
    "        df = df.merge(df_2, on=['dd_id'], how='outer')\n",
    "\n",
    "    if 'post_date' in df:\n",
    "        df['post_date'] = df.post_date.apply(pd.to_datetime)\n",
    "        \n",
    "    if 'duration_in_mins' in df:\n",
    "        df['duration_in_mins'] = df.duration_in_mins.apply(lambda x: float(x) if x != '' else np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsns = [json.loads(x) for x in open('../data/orig_ht_data/ht_training.json', 'r')]\n",
    "train_df = pd.DataFrame.from_records(jsns)\n",
    "train_df = train_df.ix[:, ['class', 'phone', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4796, 3)\n",
      "(4192, 3)\n",
      "(4169, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df['phone'] = train_df.phone.apply(lambda x:','.join(x))\n",
    "train_df = disaggregated_df(train_df, 'phone', ',')\n",
    "print(train_df.shape)\n",
    "train_df = train_df.ix[train_df.phone != 'nan', :]\n",
    "print(train_df.shape)\n",
    "\n",
    "train_df['phone'] = train_df.phone.apply(phone_str_to_dd_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop phone numbers that have both positive and negative values. This is an oversimplification, and we could also try something like calculating a real score. This is okay for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4034, 3)\n"
     ]
    }
   ],
   "source": [
    "pos_neg_set = \\\n",
    "set(train_df.ix[(train_df['class'] == 'positive'), 'phone']) &\\\n",
    "set(train_df.ix[(train_df['class'] == 'negative'), 'phone'])\n",
    "train_df = train_df.ix[~train_df.phone.isin(pos_neg_set), :]\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using urls, so we drop them too. We can put these back if we want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.ix[:, ['class', 'phone']].drop_duplicates()\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DeepDive Data w/ Training Phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_con = create_engine('sqlite:////Users/pmlandwehr/wkbnch/memex/memex_queries/dd_dump.db')\n",
    "query_str_fmt = 'select {} from {} where {} in ({})'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31275, 2)\n",
      "(17471, 2)\n"
     ]
    }
   ],
   "source": [
    "phone_str_list = ','.join(['\"{}\"'.format(x) for x in train_df.phone.unique()])\n",
    "query_str = query_str_fmt('*', 'dd_id_to_phone', 'phone', phone_str_list)\n",
    "df = pd.read_sql(query_str, sql_con)\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16458, 7)\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_of_tables_for_dd_ids(list(df.dd_id.unique()),\n",
    "                                ['dd_id_to_price_duration',\n",
    "                                'dd_id_to_flag',\n",
    "                                'dd_id_to_age',\n",
    "                                'dd_id_to_cbsa'],\n",
    "                               sql_con)\n",
    "print(df_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those ads with prices and durations, let's add a price per minute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2['price_per_min'] = df_2.price / df_2.duration_in_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18727, 9)\n"
     ]
    }
   ],
   "source": [
    "## DD IDs in the first list are definitive and complete, so left join\n",
    "df_3 = df.merge(df_2, on='dd_id', how='left')\n",
    "print(df_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Deep Dive Data with Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_4 = train_df.merge(df_3, on='phone', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Deep Dive Data with Greg's HT Data\n",
    "## STD Data\n",
    "We rename the \"Name\" column to \"area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_df = pd.read_excel('../data/greg_correlates/std.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSA</th>\n",
       "      <th>CBSA</th>\n",
       "      <th>Name</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Year</th>\n",
       "      <th>Cases</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31000US12060</td>\n",
       "      <td>12060</td>\n",
       "      <td>Atlanta-Sandy Springs-Roswell, GA</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>20337</td>\n",
       "      <td>370.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31000US12420</td>\n",
       "      <td>12420</td>\n",
       "      <td>Austin-Round Rock, TX</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>8456</td>\n",
       "      <td>495.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31000US12580</td>\n",
       "      <td>12580</td>\n",
       "      <td>Baltimore-Columbia-Towson, MD</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>12883</td>\n",
       "      <td>478.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31000US13820</td>\n",
       "      <td>13820</td>\n",
       "      <td>Birmingham-Hoover, AL</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>6120</td>\n",
       "      <td>541.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31000US14460</td>\n",
       "      <td>14460</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>Chlamydia</td>\n",
       "      <td>2009</td>\n",
       "      <td>13285</td>\n",
       "      <td>289.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MSA   CBSA                               Name    Disease  Year  \\\n",
       "0  31000US12060  12060  Atlanta-Sandy Springs-Roswell, GA  Chlamydia  2009   \n",
       "1  31000US12420  12420              Austin-Round Rock, TX  Chlamydia  2009   \n",
       "2  31000US12580  12580      Baltimore-Columbia-Towson, MD  Chlamydia  2009   \n",
       "3  31000US13820  13820              Birmingham-Hoover, AL  Chlamydia  2009   \n",
       "4  31000US14460  14460     Boston-Cambridge-Newton, MA-NH  Chlamydia  2009   \n",
       "\n",
       "   Cases   Rate  \n",
       "0  20337  370.2  \n",
       "1   8456  495.9  \n",
       "2  12883  478.8  \n",
       "3   6120  541.1  \n",
       "4  13285  289.5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66675, 15)\n"
     ]
    }
   ],
   "source": [
    "df_4 = df_4.merge(std_df.ix[:, ['Name', 'Disease', 'Year', 'Cases', 'Rate', 'MSA']],\n",
    "                  left_on='area',\n",
    "                  right_on='Name',\n",
    "                  how='left')\n",
    "del df_4['Name']\n",
    "print(df_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSA Characteristics\n",
    "Note that we're using the **yearly** version of the file. We could also use the **monthly** version. I'm primarily choosing yearly because monthly had some import issues that it doesn't seem worth wrangling at this second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msa_df = pd.read_csv('../data/greg_correlates/msa_characteristics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_4 = df_4.merge(msa_df,\n",
    "                  left_on='MSA',\n",
    "                  right_on='census_msa_code',\n",
    "                 how='left')\n",
    "del df_4['census_msa_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del msa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_4.to_csv('../data/merged/data_to_use.csv', index=False)\n",
    "df_4.to_pickle('../data/merged/data_to_use.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
